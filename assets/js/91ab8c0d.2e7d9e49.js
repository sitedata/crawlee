"use strict";(self.webpackChunk=self.webpackChunk||[]).push([[167],{3905:(e,t,n)=>{n.d(t,{Zo:()=>p,kt:()=>m});var a=n(7294);function i(e,t,n){return t in e?Object.defineProperty(e,t,{value:n,enumerable:!0,configurable:!0,writable:!0}):e[t]=n,e}function r(e,t){var n=Object.keys(e);if(Object.getOwnPropertySymbols){var a=Object.getOwnPropertySymbols(e);t&&(a=a.filter((function(t){return Object.getOwnPropertyDescriptor(e,t).enumerable}))),n.push.apply(n,a)}return n}function o(e){for(var t=1;t<arguments.length;t++){var n=null!=arguments[t]?arguments[t]:{};t%2?r(Object(n),!0).forEach((function(t){i(e,t,n[t])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(n)):r(Object(n)).forEach((function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(n,t))}))}return e}function l(e,t){if(null==e)return{};var n,a,i=function(e,t){if(null==e)return{};var n,a,i={},r=Object.keys(e);for(a=0;a<r.length;a++)n=r[a],t.indexOf(n)>=0||(i[n]=e[n]);return i}(e,t);if(Object.getOwnPropertySymbols){var r=Object.getOwnPropertySymbols(e);for(a=0;a<r.length;a++)n=r[a],t.indexOf(n)>=0||Object.prototype.propertyIsEnumerable.call(e,n)&&(i[n]=e[n])}return i}var s=a.createContext({}),u=function(e){var t=a.useContext(s),n=t;return e&&(n="function"==typeof e?e(t):o(o({},t),e)),n},p=function(e){var t=u(e.components);return a.createElement(s.Provider,{value:t},e.children)},c={inlineCode:"code",wrapper:function(e){var t=e.children;return a.createElement(a.Fragment,{},t)}},d=a.forwardRef((function(e,t){var n=e.components,i=e.mdxType,r=e.originalType,s=e.parentName,p=l(e,["components","mdxType","originalType","parentName"]),d=u(n),m=i,k=d["".concat(s,".").concat(m)]||d[m]||c[m]||r;return n?a.createElement(k,o(o({ref:t},p),{},{components:n})):a.createElement(k,o({ref:t},p))}));function m(e,t){var n=arguments,i=t&&t.mdxType;if("string"==typeof e||i){var r=n.length,o=new Array(r);o[0]=d;var l={};for(var s in t)hasOwnProperty.call(t,s)&&(l[s]=t[s]);l.originalType=e,l.mdxType="string"==typeof e?e:i,o[1]=l;for(var u=2;u<r;u++)o[u]=n[u];return a.createElement.apply(null,o)}return a.createElement.apply(null,n)}d.displayName="MDXCreateElement"},4959:(e,t,n)=>{n.d(t,{Z:()=>l});var a=n(7294),i=n(9960),r=n(4477),o=n(2263);const l=function(e){var t=e.to,n=e.children,l=(0,r.E)();return(0,o.default)().siteConfig.presets[0][1].docs.disableVersioning?a.createElement(i.default,{to:"/api/"+t},n):a.createElement(i.default,{to:"/api/"+("current"===l.version?"next":l.version)+"/"+t},n)}},5690:(e,t,n)=>{n.r(t),n.d(t,{assets:()=>c,contentTitle:()=>u,default:()=>k,frontMatter:()=>s,metadata:()=>p,toc:()=>d});var a=n(7462),i=n(3366),r=(n(7294),n(3905)),o=n(4959),l=["components"],s={id:"enqueue-links",title:"Using Crawlee to enqueue links like a boss",sidebar_label:"Enqueuing links",description:"Your first steps into the world of scraping with Crawlee"},u=void 0,p={unversionedId:"introduction/enqueue-links",id:"introduction/enqueue-links",title:"Using Crawlee to enqueue links like a boss",description:"Your first steps into the world of scraping with Crawlee",source:"@site/../docs/introduction/04-enqueue-links.mdx",sourceDirName:"introduction",slug:"/introduction/enqueue-links",permalink:"/docs/introduction/enqueue-links",draft:!1,tags:[],version:"current",lastUpdatedBy:"renovate[bot]",lastUpdatedAt:1658627499,formattedLastUpdatedAt:"Jul 24, 2022",sidebarPosition:4,frontMatter:{id:"enqueue-links",title:"Using Crawlee to enqueue links like a boss",sidebar_label:"Enqueuing links",description:"Your first steps into the world of scraping with Crawlee"},sidebar:"docs",previous:{title:"CheerioCrawler",permalink:"/docs/introduction/cheerio-crawler"},next:{title:"Real-world example",permalink:"/docs/introduction/realworld-example"}},c={},d=[{value:"Introduction to <code>enqueueLinks()</code>",id:"introduction-to-enqueuelinks",level:3},{value:"Basic usage of <code>enqueueLinks()</code> with <code>CheerioCrawler</code>",id:"basic-usage-of-enqueuelinks-with-cheeriocrawler",level:3},{value:"Using <code>enqueueLinks()</code> to filter links",id:"using-enqueuelinks-to-filter-links",level:4},{value:"Integrating <code>enqueueLinks()</code> into our crawler",id:"integrating-enqueuelinks-into-our-crawler",level:4}],m={toc:d};function k(e){var t=e.components,n=(0,i.Z)(e,l);return(0,r.kt)("wrapper",(0,a.Z)({},m,n,{components:t,mdxType:"MDXLayout"}),(0,r.kt)("p",null,"If you were paying attention carefully in the previous chapter, we said there is much easier way to enqueue new ",(0,r.kt)(o.Z,{to:"core/class/Request",mdxType:"ApiLink"},(0,r.kt)("inlineCode",{parentName:"p"},"Request"))," objects with a single function call. You might be wondering why you had to go through the whole process of getting the individual links, filtering the same domain ones and then manually enqueuing them into the ",(0,r.kt)("inlineCode",{parentName:"p"},"RequestQueue"),", when there is a simpler way."),(0,r.kt)("p",null,"Well, the obvious reason is practice. This is a tutorial after all. The other reason is to make you think about all the bits and pieces that come together, so that in the end, a new page, not previously entered in by you, can be scraped. We think that by seeing the bigger picture, you will be able to get the most out of Crawlee."),(0,r.kt)("h3",{id:"introduction-to-enqueuelinks"},"Introduction to ",(0,r.kt)("inlineCode",{parentName:"h3"},"enqueueLinks()")),(0,r.kt)("p",null,"Since enqueuing new links to crawl is such an integral part of web crawling, we created a function that attempts to simplify this process as much as possible. With a single function call, it allows you to find all the links on a page that match specified criteria and add them to a ",(0,r.kt)("inlineCode",{parentName:"p"},"RequestQueue"),". It also allows you to modify the resulting requests to match your crawling needs."),(0,r.kt)("p",null,(0,r.kt)("inlineCode",{parentName:"p"},"enqueueLinks")," is quite a powerful function so, like crawlers, it gets its arguments from the ",(0,r.kt)("inlineCode",{parentName:"p"},"options")," object. This is useful, because you don't have to remember their order! But also because we can easily extend its API and add new features. You can ",(0,r.kt)(o.Z,{to:"core/function/enqueueLinks",mdxType:"ApiLink"},"find the full reference here"),"."),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-ts"},"import { enqueueLinks } from 'crawlee';\n\n// Now you can use enqueueLinks like this:\nawait enqueueLinks({\n    /* options */\n});\n")),(0,r.kt)("h3",{id:"basic-usage-of-enqueuelinks-with-cheeriocrawler"},"Basic usage of ",(0,r.kt)("inlineCode",{parentName:"h3"},"enqueueLinks()")," with ",(0,r.kt)("inlineCode",{parentName:"h3"},"CheerioCrawler")),(0,r.kt)("p",null,"We already implemented logic that takes care of enqueueing new links to a ",(0,r.kt)("inlineCode",{parentName:"p"},"RequestQueue")," in the previous chapter on ",(0,r.kt)("inlineCode",{parentName:"p"},"CheerioCrawler"),". Let's look at that logic and implement the same functionality using ",(0,r.kt)("inlineCode",{parentName:"p"},"enqueueLinks()"),"."),(0,r.kt)("p",null,"We found that the crawler needed to do these 4 things to crawl ",(0,r.kt)("inlineCode",{parentName:"p"},"apify.com")," website:"),(0,r.kt)("ol",null,(0,r.kt)("li",{parentName:"ol"},"Find new links on the page"),(0,r.kt)("li",{parentName:"ol"},"Filter only those pointing to ",(0,r.kt)("inlineCode",{parentName:"li"},"apify.com")),(0,r.kt)("li",{parentName:"ol"},"Enqueue them to the ",(0,r.kt)("inlineCode",{parentName:"li"},"RequestQueue")),(0,r.kt)("li",{parentName:"ol"},"Scrape the newly enqueued links")),(0,r.kt)("p",null,"Using ",(0,r.kt)("inlineCode",{parentName:"p"},"enqueueLinks()")," we can squash the first 3 into a single function call. We will use the context bound ",(0,r.kt)("inlineCode",{parentName:"p"},"enqueueLinks")," variant:"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-ts"},"import { CheerioCrawler } from 'crawlee';\n\nconst crawler = new CheerioCrawler({\n    async requestHandler({ enqueueLinks }) {\n        await enqueueLinks();\n    },\n});\n\nawait crawler.run(['https://apify.com']);\n")),(0,r.kt)("p",null,"Wait, what? Yes! That's all we need to find and enqueue all the links on currently handled page. We are using the context bound variant (the one from the ",(0,r.kt)("inlineCode",{parentName:"p"},"requestHandler"),"'s first context parameter), so it already knows what the current request body looks like, how to extract the links from it, how to handle relative links, it uses the crawler's ",(0,r.kt)("inlineCode",{parentName:"p"},"RequestQueue")," (and creates a default one if you don't provide your own). By default, it will enqueue only links that target the same hostname (via the so-called ",(0,r.kt)(o.Z,{to:"core/enum/EnqueueStrategy#SameHostname",mdxType:"ApiLink"},(0,r.kt)("inlineCode",{parentName:"p"},"SameHostname"))," ",(0,r.kt)(o.Z,{to:"core/enum/EnqueueStrategy",mdxType:"ApiLink"},"EnqueueStrategy"),")."),(0,r.kt)("h4",{id:"using-enqueuelinks-to-filter-links"},"Using ",(0,r.kt)("inlineCode",{parentName:"h4"},"enqueueLinks()")," to filter links"),(0,r.kt)("p",null,"While the defaults for ",(0,r.kt)("inlineCode",{parentName:"p"},"enqueueLinks")," can be often exactly what you need, it also gives you fine-grained control over ",(0,r.kt)("em",{parentName:"p"},"what")," links should be enqueued. One way we already mentioned above is by using the ",(0,r.kt)(o.Z,{to:"core/enum/EnqueueStrategy",mdxType:"ApiLink"},(0,r.kt)("inlineCode",{parentName:"p"},"EnqueueStrategy")),". You can use the ",(0,r.kt)(o.Z,{to:"core/enum/EnqueueStrategy#All",mdxType:"ApiLink"},(0,r.kt)("inlineCode",{parentName:"p"},"All"))," strategy if you want to follow every single link, regardless of its domain, or you can enqueue links that target the same domain name with the ",(0,r.kt)(o.Z,{to:"core/enum/EnqueueStrategy#SameDomain",mdxType:"ApiLink"},(0,r.kt)("inlineCode",{parentName:"p"},"SameDomain"))," strategy."),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-ts"},"await enqueueLinks({\n    strategy: EnqueueStrategy.SameDomain, // or 'same-domain'\n});\n")),(0,r.kt)("p",null,"For even more control, you can use ",(0,r.kt)("inlineCode",{parentName:"p"},"globs"),", ",(0,r.kt)("inlineCode",{parentName:"p"},"regexps")," and ",(0,r.kt)("inlineCode",{parentName:"p"},"pseudoUrls")," to filter the URLs. Each of those arguments is always an ",(0,r.kt)("inlineCode",{parentName:"p"},"Array"),", but the contents can take on many forms. ",(0,r.kt)(o.Z,{to:"core/interface/EnqueueLinksOptions",mdxType:"ApiLink"},"See the reference")," for more information about them as well as other options."),(0,r.kt)("blockquote",null,(0,r.kt)("p",{parentName:"blockquote"},"If you provide one of those options, the default ",(0,r.kt)("inlineCode",{parentName:"p"},"SameHostname")," strategy will ",(0,r.kt)("strong",{parentName:"p"},"not")," be applied unless explicitly set in the options.")),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-ts"},"await enqueueLinks({\n    globs: ['http?(s)://apify.com/*/*'],\n});\n")),(0,r.kt)("p",null,"To have absolute control, we have the ",(0,r.kt)(o.Z,{to:"core/interface/EnqueueLinksOptions/#transformRequestFunction",mdxType:"ApiLink"},(0,r.kt)("inlineCode",{parentName:"p"},"transformRequestFunction")),". Just before a new ",(0,r.kt)(o.Z,{to:"core/class/Request",mdxType:"ApiLink"},(0,r.kt)("inlineCode",{parentName:"p"},"Request"))," is constructed and enqueued to the ",(0,r.kt)(o.Z,{to:"core/class/RequestQueue",mdxType:"ApiLink"},(0,r.kt)("inlineCode",{parentName:"p"},"RequestQueue")),", this function can be used to skip it or modify its contents such as ",(0,r.kt)("inlineCode",{parentName:"p"},"userData"),", ",(0,r.kt)("inlineCode",{parentName:"p"},"payload")," or, most importantly, ",(0,r.kt)("inlineCode",{parentName:"p"},"uniqueKey"),". This is useful when you need to enqueue multiple requests to the queue, and these requests share the same URL, but differ in methods or payloads. Another use case is to dynamically update or create the ",(0,r.kt)("inlineCode",{parentName:"p"},"userData"),"."),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-ts"},"await enqueueLinks({\n    globs: ['http?(s)://apify.com/*/*'],\n    transformRequestFunction(req) {\n        // ignore all links ending with `.html`\n        if (req.url.endsWith('.html')) return false;\n        return req;\n    },\n});\n")),(0,r.kt)("h4",{id:"integrating-enqueuelinks-into-our-crawler"},"Integrating ",(0,r.kt)("inlineCode",{parentName:"h4"},"enqueueLinks()")," into our crawler"),(0,r.kt)("p",null,"Now let's get back to our crawler. Let's take a look at the original crawler code, where we enqueued all the links manually."),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-ts"},"import { URL } from 'url';\nimport { CheerioCrawler } from 'crawlee';\n\n// Set up the crawler, passing a single options object as an argument.\nconst crawler = new CheerioCrawler({\n    maxRequestsPerCrawl: 20,\n    async requestHandler({ request, $ }) {\n        const title = $('title').text();\n        console.log(`The title of \"${request.url}\" is: ${title}.`);\n\n        // Here starts the new part of requestHandler.\n        const links = $('a[href]')\n            .map((i, el) => $(el).attr('href'))\n            .get();\n\n        const { origin } = new URL(request.loadedUrl);\n        const absoluteUrls = links.map(link => new URL(link, request.loadedUrl));\n\n        const sameDomainLinks = absoluteUrls\n            .filter(url => url.origin === origin)\n            .map(url => ({ url: url.href} ));\n\n        console.log(`Enqueueing ${sameDomainLinks.length} URLs.`);\n        await crawler.addRequests(sameDomainLinks);\n    },\n});\n\nawait crawler.run(['https://apify.com']);\n")),(0,r.kt)("p",null,"Since we've already learned that the context bound ",(0,r.kt)("inlineCode",{parentName:"p"},"enqueueLinks()")," function does exactly what we need, we can just replace all the above enqueuing logic with a single function call, as promised."),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-ts"},"import { CheerioCrawler } from 'crawlee';\n\nconst crawler = new CheerioCrawler({\n    maxRequestsPerCrawl: 20,\n    async requestHandler({ request, $, enqueueLinks }) {\n        const title = $('title').text();\n        console.log(`The title of \"${request.url}\" is: ${title}.`);\n\n        const enqueued = await enqueueLinks();\n        console.log(`Enqueued ${enqueued.length} URLs.`);\n    },\n});\n\nawait crawler.run(['https://apify.com']);\n")),(0,r.kt)("p",null,"And that's it! No more parsing the links from HTML using Cheerio, filtering them and enqueueing them one by one. It all gets done automatically! ",(0,r.kt)("inlineCode",{parentName:"p"},"enqueueLinks()")," is just one example of Crawlee's powerful helper functions. They're all designed to make your life easier, so you can focus on getting your data, while leaving the mundane crawling management to the tools."),(0,r.kt)("p",null,(0,r.kt)("inlineCode",{parentName:"p"},"enqueueLinks()")," has a lot more tricks up its sleeve. Make sure to check out the ",(0,r.kt)(o.Z,{to:"core/interface/EnqueueLinksOptions",mdxType:"ApiLink"},"options reference")," to see what else it can do for you. Namely the feature to prepopulate the ",(0,r.kt)("inlineCode",{parentName:"p"},"Request")," instances it creates with ",(0,r.kt)("inlineCode",{parentName:"p"},"userData")," of your choice is extremely useful!"))}k.isMDXComponent=!0}}]);